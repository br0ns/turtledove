
\section{Parsers}

The SML and Rule parsers are both implemented in SML-Lex and SML-Yacc. We say
SML-Lex/Yacc as both the sml/nj version ml-lex/ml-yacc and mlton version
mllex/mlyacc works, but the mosml version mosmllex and mosmlyac are having
problems.

\fixme{fix the above gibberish, add something about the MLB}

\subsection{ML Basis files}

The MLB parser parses the files in two steps, First is a regular parse and the
second parse pulls out information about which files were included, etc. so any
MLB files referenced may also be parsed and their AST inserted.  ... bla blah

\fixme{Write this section}

\subsection{Standard ML}

A fine note about where the grammar was stolen from and possibly some other
clever stuff.

\fixme {Write this section}

\subsection{Rule}

The Rule parser is based on the SML parser with some added tokens to the lexer
and rules to the grammar to handle the rule syntax (see
\fref{tab:rule-grammar}). 


\subsubsection{Unicode}

The rule syntax uses the symbols \texttt{£} and \texttt{§} to denote
transformers and meta patterns respectively. These were a bit tricky to
implement as they are not ASCII characters. By default SML-Lex\cite{ml-lex-yacc}
only supports 8-bit characters and thus if the upper parts of UTF-8 characters
are needed they must be specially handled. One way to handle it would be to
convert the input file into some fixed length encoding (i.e., UTF-32), but as
SML only uses ASCII chars that would demand a total remake of the Rule lexer
definition. Instead we went with a UTF-8 solution which only need to be
specially fitted for the transformer and meta patterns as it seems that most
editors now a days use UTF-8 as default encoding (seems to be the case for emacs
on linux). Implementing the two UTF-8 values in the lexer was then just a matter
of defining a named expression containing the conjunction of the two decimal
values making up each of the UTF-8 values (see \fref{tab:utf8-rule-values}).

\begin{table}
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Letter} & \textbf{UTF-8} & \textbf{Hex} & \textbf{Decimal} \\ \hline
    Pound sign (£)   & U+00A3 & 0xC2 0xA3 &  $194$ $163$ \\ \hline
    Section sign (§) & U+00A7 & 0xC2 0xA7 & $194$ $167$ \\ \hline
  \end{tabular}

  \caption{Table of UTF-8 and hex values of \texttt{£} and \texttt{§}}
  \label{tab:utf8-rule-values}
\end{table}

As we are using the internal SML-Lex position feature \texttt{yypos} we have to
decrement it by one each time a transformer or meta pattern is encountered as it
is counted as two characters, where it actually is just one (composite)
character.

\subsection{Abstract syntax tree}

It seems that there exists one common way of representing an AST; Group
different parts of the language together in different data types, for example
patterns and expressions. In general it seems that the decision of how to break
up the language into different data types are based heavily on the BNF. This way
of representing the AST gives integrity safety for free, as for example an
\synt{exp} would not be able to contain an \synt{atpat}, which is most likely
the main motivation of doing it this way. However various examples are given in
\cite{mbp08} of how this way of representing the AST blows up code
vise when working with it as lots of almost identical functions are needed to
work on all the different data types making up the AST. Because of this we have
chosen a dramatical new approach\footnote{Compared to how all the SML
  interpresers/compilers such as SML/NJ, MLKit and MLton} as to how the AST is
represented; We have chosen to represent it using only one data type. This
doesn't give any gives integrity but it gives a great deal of flexibility
(``code by convention''). Such an integrity check seems to be a small ``price to
pay'' compared to the extra flexibility and what seems to be less code to work
with the AST.


This way of thinking generalised even further. Both the SML parser and Rule
parser uses the same AST, as this will make it much easier when comparing actual
SML code to a scheme in the defined rules.


\subsubsection{wrap}

Actually the AST is not just a n-ary tree structure of the above mentioned
single data type. Each node in the AST is a \textit{wrap} of a token (the data
type) and a left and right field both of type $\alpha$. These two fields, left
and right, are polymorph such that any manipulations to the AST can transform it
from some $\alpha$ to some $\beta$. 

Initially, out of the parser, the left and right fields are integer character
positions of left- and rightmost part of the text representing the token
including any children. This is later changed in the infix resolving to also
contain infix status and environment information, where the environment
information is that just before, in the left part, and the information after
this node, in the right part.

\fixme{give a reference to where all this is explained in detail}




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../rewriting-syntax"
%%% End: 

